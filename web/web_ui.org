#+NAME: APISnoop WebUI Overview
#+AUTHOR: Zach Mandeville
#+EMAIL: zz@ii.coop
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(i) | DONE(d) DONE-AND-SHARED(!)
#+PROPERTY: header-args :dir (file-name-directory buffer-file-name)
#+XPROPERTY: header-args:shell :results silent
#+XPROPERTY: header-args:shell :exports code
#+XPROPERTY: header-args:shell :wrap "SRC text"
#+PROPERTY: header-args:tmate :socket "/tmp/.zz-left.isocket"
#+PROPERTY: header-args:tmate :session api:main

* Navigating This Directory
The =/web= directory contains all the code for our web interface, from the backend server that hosts the data to the frontend client that renders the data into sunbursts.

Both the backend and frontend code were written as org files first and then tangled into files and folders.  This means that the best way to navigate either is to read their individual =.org=, held in the root of their respective directories.


This org is intended to give a broad overview of how the site functions so you can better orient yourself within the code, and better understand where your next great feature should be placed!

* Architecture
  The application is designed to be modular, with no direct connection between the front and backend.  The hope was we could choose different implmenetations for either as our understanding improves, and it wouldn't break the entire site.

However, there is still an assumed architecture for the site that affects what we implement and how.  The architecture makes these assumptions:
  - The backend creates an API, the client consumes the API
  - This API is built out of processed apisnoop data.
  - This api is not organized by release, but instead =/endpoints=, =tests=, =useragents=, and =releases=.
  - These API endpoints contain data across all releases, but can be selectively fetched depending on the client's needs.

The location of this api is not important.  Currently, our backend both creates a server and api simultaneously, which we set to a distinct url that we set again in the client.  But as long as there's a location, and a custom api, the client does not care how it's made.

** Backend Creates API
   You can view the org specific to the backend at [[file:backend/backend.org][./backend/backend.org]] .  This will serve as a broad overview.

   Our backend takes processed apisnoop data held across multiple files and folders and turns it into an API which it serves at a set url.  That is really its only function.  There is little processing or logic beyond that initial big-bang, though this big-bang does create some calculated fields and organization (namely within the endpoint service).

  We are using [[https://docs.feathersjs.com/][FeathersJS]] to implement this currently.  It was chosen because of its endorsement from Enspiral friends in Wellington, its general simplicity to get set up (especially with its CLI too), and its existing client-connector that works seamlessly with react.

  Our current implementation holds these assumptions:
  - The data it needs to create an api will be held within a specific local folder.
  - This data will consist of multiple folders, each containing three files: =apisnoop.json=, =metadata.json=, =finished.json=
  - These folders will be organized consistently within a single parent folder.
  - The files will hold consistently structured json that includes information about endpoints, tests, useragents, and releases.

These assumptions are potentially trivial, in that changing just the config functions could have the backend fetch from another URL instead of local files.  It is still built to turn the json it fetches into client-ready api and be able to serve it.

** Client consumes API
You can view the org file specific to the client at [[file:client/client.org][./client/client.org]]

The client does all the heavywork of our site.  All logic for filtering, fetching, displays, search, etc. are handled by the client.  It's only interaction with the backend is to make an initial fetch of data based on whichever release has been selected.

The location of the backend is set as an .env variable.  The client does not care about the path just as long as it can be reached by https and that it will be structured as outlined above.

The client doesn't make additional assumptions about the data, but has these assumptions about its primary audience:
  - They will be coming from the kubernetes development community
  - They will be using high-speed internet with consistent connections
  - They will be visiting the site on their desktop instead of their phone
  - They will want to share insights gained by the site with others, using a url.

The client is built with [[https://reactjs.org/][React]] and [[https://redux.js.org/][Redux(]]via reduxbundler).   The heart of it is really the redux bundles, with React acting as just presentational components for the data.

React was chosen because of my personal familiarity with it and access to support in Wellington.  Since it is implemented intentionally simply it would be relatively simple to use a different component framework.

Redux was chosen also for personal familiarity and its tremendous power in holding state and organizing different sets of intersecting data.  I really dig it and it would be non-trivial to choose a different path (it would require a heavy re-architecting at least).

Lastly, the CSS for the site is done with [[http://tachyons.io/docs/][Tachyons.]]  This is a library that follows a functional, unix-inspired way to create CSS classes.  It's great when you are designing a site component by component, and are maintaining it with a team as you only care about what classes are attributed to a component, and not the properties of the class.

* Setting up Dev Environment
** Clone The Repository, and install apinsoop.sh dependencies
   #+NAME: Clone Repo and Setup Dependencies
   #+BEGIN_SRC shell
     git clone git git@github.com:cncf/apisnoop.git
     cd apisnoop
     ./apisnoop.sh --install
     ./apisnoop.sh --download-apiusage
   #+END_SRC

   our helper script will download from the apisnoop gcs bucket to =data-gen/processed=, where the backend knows to find it.
** Setup Backend
   #+NAME: Setup Backend and Install Dependencies
   #+BEGIN_SRC shell
     #from within apisnoop
     cd web/backend
     npm install
     npm start
   #+END_SRC

   This should start it at port =3030=.  If you need a different port, you can change it in =backend/config/default.json=.
Alternatively, you can set up different ports per working environment.

So if I wanted a =zach= environment I would copy default.json to a file called =zach.json=, and change it's port value to whatever port I want.

Then, I would set the NODE_ENV for the backend to =zach=.

#+NAME: Set Node Environment to zach
#+BEGIN_SRC
# Within web/backend and with the server process stopped
export NODE_ENV=zach
npm start
#+END_SRC

** Setup Client
   #+NAME: enter client and install dependencies
   #+BEGIN_SRC shell
     #from within apisnoop
     cd web/client
     npm install
     npm start
   #+END_SRC

   Similar to  the backend, you can setup the environment to point to whichever port for your backend you'd like.  You do this by editing the =.env= file within =web/client=

   #+NAME: .env Example
   #+BEGIN_EXAMPLE js
  REACT_APP_BACKEND_URL= http://localhost:3030
   #+END_EXAMPLE

   When this changes you will want to stop the client process and restart it with =npm start=

* Resources
  There are add'l resources in the client.org and backend.org
** Feathers Resources
*** [[https://www.youtube.com/playlist?list=PLwSdIiqnDlf_lb5y1liQK2OW5daXYgKOe][youtube channel]]
*** [[https://stackoverflow.com/questions/tagged/feathersjs][stackoverflow tag]]
*** [[https://github.com/issues?utf8=%25E2%259C%2593&q=is%253Aopen+is%253Aissue+user%253Afeathersjs+][github page]]
*** [[https://blog.feathersjs.com/][medium page]]
*** [[http://slack.feathersjs.com/][slack channel]]

** d3
*** [[https://medium.com/@Elijah_Meeks/interactive-applications-with-react-d3-f76f7b3ebc71][interactive applications with react-d3]]
    this is really good.
*** [[https://www.smashingmagazine.com/2018/02/react-d3-ecosystem/][Bringing Together react, d3, and their ecosystem]]
*** [[http://www.adeveloperdiary.com/react-js/integrate-react-and-d3/][How to Integrate React and d3 the right way]]
*** [[https://bost.ocks.org/mike/join/][Thinking with Joins]]
** react/redux
   - [[https://read.reduxbook.com][Human Redux, by Henrik Joreteg]]


* Archived Notes
** isocket :ARCHIVE:
*** Connecting the left pair / isocket

 ssh needs '-t' twice because it needs to be forced to allocate a remote terminal
 _even_ when we don't have have local one (within emacs)


#+NAME: left_session_create
#+BEGIN_SRC shell :var session="zz-left" terminal_exec="xterm -e" user="zz" host="apisnoop.cncf.io" :session nil :results silent
  $terminal_exec \
      "ssh -att \
           -L /tmp/.$session.isocket:/tmp/.$session.isocket \
           -l $user \
           $host \
      tmate -S /tmp/.$session.isocket \
            new-session \
            -A \
            -s $session \
            -n emacs \
      emacs --fg-daemon=$session" \
  &
#+END_SRC

#+NAME: left_session_setup
#+BEGIN_SRC shell :var session="zz-left" user="zz" host="apisnoop.cncf.io" :session nil :results silent
  ssh -att $user@$host \
  "tmate -S /tmp/.$session.isocket \
        new-window \
        -n client" \
   "emacsclient -nw \
              --socket-name $session \
              ~/apisnoop/webui/web_ui.org"
#+END_SRC

 #+NAME: left_session
 #+BEGIN_SRC shell :wrap "SRC text :noeval" :results verbatim :var session="zz-left" user="zz" host="apisnoop.cncf.io" :results silen
  ssh -att $user@$host \
    tmate -S /tmp/.$SESSION.isocket wait tmate-ready > /dev/null &&
  ssh -att $user@$host \
    tmate -S /tmp/.$SESSION.isocket display -p \'#{tmate_ssh}\' 2> /dev/null
# ssh -tt root@apisnoop.cncf.io \
#  tmate -S /tmp/.$SESSION.isocket display -p \'#{tmate_ssh}\'
 #+END_SRC

 #+RESULTS: left_session
 #+BEGIN_SRC text :noeval
 #+END_SRC

**** Connecting to emacs daemon

 #+NAME: alse run emacsclient
 #+BEGIN_SRC tmate :noeval
 export SESSION=lt-emacs
 emacsclient --socket-name $SESSION
 #+END_SRC

*** Connecting the right pair / isocket

#+NAME: right_session_create
#+BEGIN_SRC shell :var session="zz-right" terminal_exec="xterm -e" user="zz" host="apisnoop.cncf.io" :session nil :results silent
  $terminal_exec \
      "ssh -att \
           -L /tmp/.$session.isocket:/tmp/.$session.isocket \
           -l $user \
           $host \
      tmate -S /tmp/.$session.isocket \
            new-session \
            -A \
            -s $session \
            -n misc" \
  &
#+END_SRC


 #+NAME: right_session_join
 #+BEGIN_SRC shell :results silent
 export SESSION=api-snoop
 export XTERM_EXEC="roxterm -e"
 $XTERM_EXEC ssh -Att root@apisnoop.cncf.io \
  tmate -S /tmp/.$SESSION.isocket \
   at \; sleep 9999
 #+END_SRC

 #+NAME: right_session_setup
 #+BEGIN_SRC shell :results verbatim
 export SESSION=api-snoop
 echo ssh -tt root@apisnoop.cncf.io \
  tmate -S /tmp/.$SESSION.isocket \
    new-window -n session \
     bash
 #+END_SRC

 #+NAME: right_session
 #+BEGIN_SRC shell :cache yes :wrap "SRC text :noeval" :results verbatim
 export SESSION=api-snoop
 ssh -tt root@apisnoop.cncf.io \
  tmate -S /tmp/.$SESSION.isocket display -p \'#{tmate_ssh}\'
 #+END_SRC

 #+RESULTS[dd96525b42bbbe741e292e99ad5f3592a7163025]: right_session
 #+BEGIN_SRC text :noeval
 ssh mJrsCgvGTOTOFagYpBKvRf7EE@sf2.tmate.io
 #+END_SRC





 #+NAME: give this to your pair
 #+BEGIN_SRC bash :noweb yes :var left_session=left_session() right_session=right_session()
 echo "ii pair session ready
 left: $left_session
 right: $right_session
 "
 #+END_SRC

 #+RESULTS: give this to your pair
 | ii     | pair | session | ready |
 | left:  | nil  |         |       |
 | right: | nil  |         |       |
 |        |      |         |       |

*** TODO Sharing your eyes

#+NAME: give this to your pair
#+BEGIN_SRC bash :noweb yes :var left_session=left_session() :var right_session=right_session()
echo "ii pair session ready
left: $left_session
right: $right_session
"
#+END_SRC
** Working with d3 :ARCHIVE:
*** Introduction
   d3 is the data visualization library that was used to make our original sunburst.  The way it works is to mount itself to the dom, and then appends new elements to the dom based on the data it was given. If that data changes, it transforms the elements as needed.

   The way react works is it attaches itself to the dom, then creates a //shadow dom// that it is continually listening to, adding and removing elements in this dom as needed based on the data(the state) it was given.

   In other words, they work in largely the same way, and both wanna attach themselves to the dom and manipulate it.  This...isn't good.  We want to have /1/ thing making shadow doms and calls on the website, and so it is a bit tricky to get react and d3 working together.

The upside is that a number of people have tackled this challenge and created different react/d3 libraries for how the two can work together.  The downside is that I'm not sure yet which is the best to do.

Put simply, it is not easy to take our existing sunburst code and just paste it into our new app.  We are going to need to transform it in some way based on the guidance of the library we are using.

So the question is why we are putting ourselves into this trouble?
*** WHY WE ARE PUTTING OURSELVES INTO THIS TROUBLE
    My assumption with all of this is that when people hear 'apisnoop', they are thinking of the site in which you can see the data visualziations.  And so the webapp is important for the project and will be expanded.  React would be great for this in the long run.

Similarly, I am expecting that we are going to have more types of visualizations than just the sunburst--and that even the sunburst may change.  So we are going to want to have an understanding for a language in which we can make a //bunch// of visualizations. d3 is great for this.

If we do it right, we can have reusable components too that other teams could use for their own k8s projects, and that we could use ourselves.  For example--displaying two sunburst charts side by side would be much easier in react/d3 then what i ws trying to do before (appending both to the same id on a standard html document.)  This requires that I move through some d3 tutorials though.

At the end of this, though, we will have a backend server that is easy to setup and can ping different url's (github repos or testgrid artifacts) and grab their data.  Then, we can manipulate that data in whatever way we want but also pass it along to our frontend.  This front-end can then have different options and tags setto really dive in and explore.

If this is the purpose of apisnoop then let's do it.  If it's too much overkill though, then I can try a simpler solution.
*** Possible Process to get going
**** Setup a simple d3 visualization to understand the process
**** pipe data into this simple visualization through our redux state.
**** Pore over the original code again (the original blog post) to see how to best convert it
**** Change the sunburst's origin point from a CSV file to JSON
**** Change the sunbursts origin point from JSON to our redux store.
*** Second Process
**** Setup different pages for different d3-react libraries that already have ubilt components.
**** explore piping our data into the one we like.
**** Use testgrid conformance data and make simple visualizations to it.
     We are wanting to keep the data retrieval tied into the visualizing, so we dont' end up with a pretty graph that can't be used for what we have.  So we can grab the testgrid stuff now and see what we can do with it.


**** Use that going forward.
*** Possible Libraries to use
**** Victory
     https://formidable.com/open-source/victory/
**** Britecharts react
     https://eventbrite.github.io/britecharts-react/
**** Recharts
     http://recharts.org/en-US/
** Aaron Feedback :ARCHIVE:
- useful troubleshooting tool:
  - adding test names to user agents to verify a test was testing what we thought it was.
  - filter audit logs by user-agent and then see 'when this test case is run, here are the endpoints it accesses chronologically".
    - This is separate from number of times hit.  that is useful in aggregate, this is something different.
  - pulling in an audit log of timestamp/verb/uri
- Feature of pointing to the specific line in the source for each test, to pull its definition, would be a good //Next// step.
  - This is something we can do with whakapapa, but it's not something we have now.
- Discovery front: Filtering more endpoints from APIsnoop's definition of coverage.
  - If beta endpoints always get hit because an api server is doing discovery, then that's cool but nothing we can ever prevent conformance tests from doing and we shouldn't care about it from a test coverage perspective.
  - How do we signify that this is the kinda hit that's happening for an endpoint?
  - We have a good start with filtering to just e2e, but even our e2etests are hitting those endpoints.  There are some endpoints where, logically they don't need to get tested or anything like that.
  - Get to a point whwere we can manually specify, or have a blacklist of apiendpoints that we aren't factoring into our coverage viz.
  - One way to do this is to filter out the endpoints that are hit by nearly all of the tests.  This is a good indicator that the endpoint is for initialization or something like that, and not actually a part of this test's function.
- Unique Endpoints hit by a test: this is something that isn't covered by our sunburst or katherine's viz.  Pick a test, and then see the endpoints that are //only// hit by this test.
  - which endpoints hit are unique, versus which ones are common across all test cases.  This would let us know which test cases are doing good stuff and which endpoints are essentially meaningless.
  - you could have a center endpoint change to the perspective of that test, and then that test would only show the endpoints that it hits.....but that may not be that useful.  We dont' wanna see All the endpoints, we wanna see which ones are //special// for this test.
  - Hierarchy vizes aren't that useful.
  - I just wanna find a way to slice and dice data with raw queries and see where that leads us...and take some of the more useful queries and generate reports from that.  This sounds like a new approach for apisnoop.

Question from this, then: Who is apisnoop's audience?  Is it Aaron,and people like aaron?  is it a kubernetes end user?  If it's aaraon, he is saying he knows how to write certain queries, but he would rather have this  already done and then he can do further exploration.
'For an endpoint that's only hit three times, what are the tests that are hitting this endpoint.  And then we could follow up with what the tests are doing from an api perspective.  'Okay, now let me see the full api stream from this test."
 - auotmate this, or provide shiny reports for this.  This isn't the end user coming up with the interesting things, this is us coming up with interesting things that we are letting the end user come to their own conclusions on.
 - We eventually want to show api coverage going up over time across different builds. o
 - We might be able to format things in such a way to have a test dashboard that shows individual api endpoints and #'s: how many times they been hit, something like that.
 - Is code coverage a different thing?  when talking about it being a command line tool that generates reports from it...or is that just what the group is trying to do.  the benefit of the command line tool is that you can automate it running for every build. We could then just have a page that displays these reports even maybe.
 - We want to share shinies at kubeconf china.
 - Get visualization up to good place that replaces existing visualization.
 - Showing all the api accesses per user-agent or test as a different Dashboard to have.
 - Take care of you for whatever demos you need for apisnoop.
 - It would be worth it to show we're providing value to cncf as a whole, but right now it's good to just be able to have Aaron say that the work we're doing makes it easier for conformance to do the things they want to do.
 - Let's not work on things that don't end up providing value, over-delivering when he really just wants somethings maller and specific.  He's happy to have some reports that don't need to be that shiny, but maybe a little bit interactive.  and these reports would be:
   - If I click on a user agent, I can see the in-order access of all the api endpoints.
   - To get some kind of report that shows me what kind of endpoints don't matter (every test hits them) and which ones are interesting (cos only a few endpoints hit them) and what are those tests?
     - this may lead to a point where we try to make a whitelist of endpoints in our coverage, but let's not cross that bridge yet.
   - For wednesday deadline...this isn't a hard deadline, we can touch base on Tuesday/Monday and see where we at.
** Pairing With Mikey :ARCHIVE:
*** Background
    I went through a pairing Session with [[https://dinosaur.is][Mikey]], to help with the overall architecture and code logic of the webui
