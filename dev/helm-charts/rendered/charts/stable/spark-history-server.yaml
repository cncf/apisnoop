---
# Source: spark-history-server/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-0.3.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
data:
  enablePVC: "true"
  enableGCS: "false"
  enableS3: "false"
  enablePVC: "true"
  eventsDir: "/"
  existingClaimName: "nfs-pvc"

---
# Source: spark-history-server/charts/nfs/templates/nfs-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
  labels:
    app.kubernetes.io/name: nfs
    helm.sh/chart: nfs-0.1.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: RELEASE-NAME-nfs.hub.svc.cluster.local
    path: "/"

---
# Source: spark-history-server/charts/nfs/templates/nfs-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
  labels:
    app.kubernetes.io/name: nfs
    helm.sh/chart: nfs-0.1.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 1Mi

---
# Source: spark-history-server/charts/nfs/templates/nfs-server-gce-pv.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: RELEASE-NAME-nfs
  labels:
    app.kubernetes.io/name: nfs
    helm.sh/chart: nfs-0.1.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 5Gi

---
# Source: spark-history-server/templates/serviceaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: RELEASE-NAME-spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-0.3.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller

---
# Source: spark-history-server/templates/rbac.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: RELEASE-NAME-spark-history-server-cr
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-0.3.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
rules:
- apiGroups: [""]
  resources: ["deployments", "pods"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: RELEASE-NAME-spark-history-server-crb
subjects:
- kind: ServiceAccount
  name: RELEASE-NAME-spark-history-server
  namespace: hub
roleRef:
  kind: ClusterRole
  name: RELEASE-NAME-spark-history-server-cr
  apiGroup: rbac.authorization.k8s.io

---
# Source: spark-history-server/charts/nfs/templates/nfs-server-service.yaml

kind: Service
apiVersion: v1
metadata:
  name: RELEASE-NAME-nfs
  labels:
    app.kubernetes.io/name: nfs
    helm.sh/chart: nfs-0.1.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    app.kubernetes.io/name: nfs
    app.kubernetes.io/instance: RELEASE-NAME

---
# Source: spark-history-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-0.3.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
  type: LoadBalancer
  ports:
  - port: 18080
    targetPort: historyport
    protocol: TCP
    name: historyport
  selector:
    app.kubernetes.io/name: spark-history-server
    app.kubernetes.io/instance: RELEASE-NAME

---
# Source: spark-history-server/charts/nfs/templates/nfs-server-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-nfs
  labels:
    app.kubernetes.io/name: nfs
    helm.sh/chart: nfs-0.1.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nfs
      app.kubernetes.io/instance: RELEASE-NAME
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nfs
        app.kubernetes.io/instance: RELEASE-NAME
    spec:
      containers:
      - name: RELEASE-NAME-nfs
        image: k8s.gcr.io/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /exports
            name: mypvc
      volumes:
        - name: mypvc
          persistentVolumeClaim:
            claimName: RELEASE-NAME-nfs

---
# Source: spark-history-server/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-spark-history-server
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-0.3.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: spark-history-server
      app.kubernetes.io/instance: RELEASE-NAME
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark-history-server
        app.kubernetes.io/instance: RELEASE-NAME
    spec:
      serviceAccountName: RELEASE-NAME-spark-history-server
      containers:
      - name: spark-history-server
        image: "lightbend/spark-history-server:2.4.0"
        imagePullPolicy: IfNotPresent
        env:
        - name: HADOOP_CONF_DIR
          value: /etc/hadoop
        - name: SPARK_NO_DAEMONIZE
          value: "true"
        ports:
        - name: historyport
          containerPort: 18080
          protocol: TCP
        command:
        - "/bin/sh"
        - "-c"
        - >
          if [ "$enablePVC" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=file:/mnt/$eventsDir";
          elif [ "$enableGCS" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/$key \
            -Dspark.history.fs.logDirectory=$logDirectory";
          elif [ "$enableS3" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.history.fs.logDirectory=$logDirectory
              -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem";
            if [ "$enableIAM" == "false" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.hadoop.fs.s3a.access.key=$(cat /etc/secrets/${accessKeyName}) \
              -Dspark.hadoop.fs.s3a.secret.key=$(cat /etc/secrets/${secretKeyName})";
            fi;
          else
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=$logDirectory";
          fi;
          /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer;
        envFrom:
        - configMapRef:
            name: RELEASE-NAME-spark-history-server
        livenessProbe:
          httpGet:
            path: /
            port: historyport
        readinessProbe:
          httpGet:
            path: /
            port: historyport
        volumeMounts:
        - name: data
          mountPath: /mnt//
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: nfs-pvc

---
# Source: spark-history-server/templates/cleanup-job.yaml
# Delete the parent chart before the sub-chart
apiVersion: batch/v1
kind: Job
metadata:
  name: RELEASE-NAME-spark-history-server
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": hook-succeeded
  labels:
    app.kubernetes.io/name: spark-history-server
    helm.sh/chart: spark-history-server-0.3.0
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/managed-by: Tiller
spec:
 template:
   spec:
     serviceAccountName: RELEASE-NAME-spark-history-server
     restartPolicy: OnFailure
     containers:
     - name: main
       image: "lightbend/curl:7.47.0"
       imagePullPolicy: IfNotPresent
       command:
         - "/bin/sh"
         - "-c"
         - "curl -ik \
          -X DELETE \
          -H 'Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)' \
          -H 'Accept: application/json' \
          -H 'Content-Type: application/json' \
          https://kubernetes.default.svc/api/v1/deployments/RELEASE-NAME-spark-history-server"

